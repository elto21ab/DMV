{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_df = pd.read_csv('data/raw/calendar2024.csv')\n",
    "lis_df = pd.read_csv('data/raw/listings2024.csv') \n",
    "rev_df = pd.read_csv('data/raw/reviews2024.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initial Data Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_dataset(df, name):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Dataset: {name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    print(\"\\n1. Basic Information:\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    \n",
    "    print(\"\\n2. Data Types:\")\n",
    "    print(df.dtypes)\n",
    "\n",
    "    print(\"\\n3. Summary Statistics:\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    print(\"\\n4. Unique Values:\")\n",
    "    for col in df.columns:\n",
    "        n_unique = df[col].nunique()\n",
    "        if n_unique < 3:\n",
    "            unique_values = df[col].unique()\n",
    "            print(f\"{col}: {n_unique} unique values\")\n",
    "            print(f\"Values: {unique_values}\\n\")\n",
    "\n",
    "    print(\"\\n5. Missing Values:\")\n",
    "    # Todo: Elias ad unique values + here\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df)) * 100\n",
    "    missing_info = pd.DataFrame({\n",
    "        'Missing Values': missing,\n",
    "        'Percentage': missing_pct\n",
    "    })\n",
    "    print(missing_info[missing_info['Missing Values'] > 0])\n",
    "    \n",
    "    return missing_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_missing = explore_dataset(lis_df, 'Listings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_missing = explore_dataset(cal_df, 'Calendar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_missing = explore_dataset(rev_df, 'Reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Missing Values Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_missing_values(missing_info, title):\n",
    "    # Filter columns with more than 0.1% missing values\n",
    "    missing_filtered = missing_info[missing_info['Percentage'] > 0.001]\n",
    "    \n",
    "    plt.figure(figsize=(18, 10))\n",
    "    ax = missing_filtered['Percentage'].plot(kind='bar')\n",
    "    plt.title(f'Missing Values in {title} Dataset')\n",
    "    plt.xlabel('Columns')\n",
    "    plt.ylabel('Percentage Missing')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    \n",
    "    # Add total missing values labels on top of each bar, rotated 45 degrees\n",
    "    for i, v in enumerate(missing_filtered['Missing Values']):\n",
    "        ax.text(i, missing_filtered['Percentage'].iloc[i], f'{int(v):,}', \n",
    "                ha='left', va='bottom', fontsize=8, rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot missing values for each dataset\n",
    "plot_missing_values(listings_missing, 'Listings')\n",
    "plot_missing_values(calendar_missing, 'Calendar')\n",
    "plot_missing_values(reviews_missing, 'Reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_data_quality_issues(df, name):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Data Quality Report for {name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # 1. Check for duplicates\n",
    "    n_duplicates = df.duplicated().sum()\n",
    "    if n_duplicates > 0:\n",
    "        print(f\"\\nDuplicate rows: {n_duplicates}\")\n",
    "    \n",
    "    # 2. Check for unexpected values\n",
    "    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    has_unexpected = False\n",
    "    for col in numeric_cols:\n",
    "        n_zeros = (df[col] == 0).sum()\n",
    "        n_negative = (df[col] < 0).sum()\n",
    "        if n_zeros > 0 or n_negative > 0:\n",
    "            if not has_unexpected:\n",
    "                print(\"\\nColumns with unexpected values:\")\n",
    "                has_unexpected = True\n",
    "            print(f\"\\n{col}:\")\n",
    "            if n_zeros > 0:\n",
    "                print(f\"- Zeros: {n_zeros} ({(n_zeros/len(df))*100:.2f}%)\")\n",
    "            if n_negative > 0:\n",
    "                print(f\"- Negative values: {n_negative} ({(n_negative/len(df))*100:.2f}%)\")\n",
    "    \n",
    "    # 3. Check string columns for data inconsistencies\n",
    "    string_cols = df.select_dtypes(include=['object']).columns\n",
    "    has_inconsistencies = False\n",
    "    for col in string_cols:\n",
    "        n_empty = (df[col] == '').sum()\n",
    "        n_whitespace = df[col].str.isspace().sum() if df[col].dtype == 'object' else 0\n",
    "        if n_empty > 0 or n_whitespace > 0:\n",
    "            if not has_inconsistencies:\n",
    "                print(\"\\nColumns with inconsistencies:\")\n",
    "                has_inconsistencies = True\n",
    "            print(f\"\\n{col}:\")\n",
    "            if n_empty > 0:\n",
    "                print(f\"- Empty strings: {n_empty}\")\n",
    "            if n_whitespace > 0:\n",
    "                print(f\"- Whitespace only: {n_whitespace}\")\n",
    "    \n",
    "    # 4. Check for extreme values in numeric columns\n",
    "    has_outliers = False\n",
    "    for col in numeric_cols:\n",
    "        mean = df[col].mean()\n",
    "        std = df[col].std()\n",
    "        outliers = df[col][abs(df[col] - mean) > 3*std]\n",
    "        if len(outliers) > 0:\n",
    "            if not has_outliers:\n",
    "                print(\"\\nColumns with outliers (beyond 3 std devs):\")\n",
    "                has_outliers = True\n",
    "            print(f\"\\n{col}:\")\n",
    "            print(f\"- Number of outliers: {len(outliers)}\")\n",
    "            print(f\"- Min outlier: {outliers.min()}\")\n",
    "            print(f\"- Max outlier: {outliers.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Listings dataset\n",
    "identify_data_quality_issues(lis_df, 'Listings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Calendar dataset\n",
    "identify_data_quality_issues(cal_df, 'Calendar')\n",
    "\n",
    "# Additional calendar-specific checks\n",
    "print(\"\\nChecking calendar date patterns:\")\n",
    "cal_df['date'] = pd.to_datetime(cal_df['date'])\n",
    "print(f\"Date range: {cal_df['date'].min()} to {cal_df['date'].max()}\")\n",
    "print(f\"Missing dates: {cal_df['date'].isnull().sum()}\")\n",
    "print(f\"Days between min and max date: {(cal_df['date'].max() - cal_df['date'].min()).days}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Reviews dataset\n",
    "identify_data_quality_issues(rev_df, 'Reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Data Format Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_formats(df):\n",
    "    \"\"\"Check for inconsistent formats within columns\"\"\"\n",
    "    for col in df.columns:\n",
    "        # Get sample of unique values\n",
    "        unique_samples = df[col].dropna().unique()[:2]\n",
    "        print(f\"\\n{col}:\")\n",
    "        for sample in unique_samples:\n",
    "            print(f\"Value: {sample}, Type: {type(sample)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_data_formats(lis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_data_formats(cal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_data_formats(rev_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Special Characters Examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_special_characters(df):\n",
    "    \"\"\"Check for special characters that might need handling\"\"\"\n",
    "    string_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in string_cols:\n",
    "        # Fixed: Properly chain the .any() method\n",
    "        if df[col].astype(str).str.contains(r'[^a-zA-Z0-9\\s\\-.,:/+&æøåÆØÅ]').any():\n",
    "            print(f\"\\n{col} contains special characters\")\n",
    "            # Show examples of rows containing special characters\n",
    "            print(df[df[col].astype(str).str.contains(r'[^a-zA-Z0-9\\s\\-.,:/+&æøåÆØÅ]')][col].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_special_characters(lis_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_special_characters(cal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_special_characters(rev_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
