{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV files\n",
    "cal = pd.read_csv('data/raw/calendar2024.csv')\n",
    "lis = pd.read_csv('data/raw/listings2024.csv') \n",
    "rev = pd.read_csv('data/raw/reviews2024.csv')\n",
    "print(\"Listings shape:\", lis.shape)\n",
    "print(\"Calendar shape:\", cal.shape) \n",
    "print(\"Reviews shape:\", rev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "        n_unique = df[col].nunique()\n",
    "        if n_unique < 3:\n",
    "            unique_values = df[col].unique()\n",
    "            print(f\"{col}: {n_unique} unique values\")\n",
    "            print(f\"Values: {unique_values}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_cols_lis = lis.columns[lis.isna().all()].tolist()\n",
    "lis = lis.drop(columns=null_cols_lis)\n",
    "\n",
    "lis.drop(columns=['scrape_id', 'host_name', 'picture_url', 'host_url', 'host_thumbnail_url', 'host_picture_url'], inplace=True)\n",
    "cal.drop(columns=['adjusted_price'], inplace=True)\n",
    "rev.drop(columns=['reviewer_name'], inplace=True)\n",
    "\n",
    "\n",
    "def convert_to_boolean(df, columns, true_value='t'):\n",
    "    \"\"\"Convert specified columns from string indicators to boolean\"\"\"\n",
    "    for col in columns:\n",
    "        df[col] = df[col] == true_value\n",
    "    return df\n",
    "\n",
    "boolean_cols = ['instant_bookable', 'host_is_superhost', 'host_has_profile_pic', 'host_identity_verified', 'has_availability']\n",
    "lis = convert_to_boolean(lis, boolean_cols)\n",
    "cal['available'] = cal['available'] == 't'\n",
    "\n",
    "\n",
    "def convert_to_datetime(df, columns):\n",
    "    \"\"\"Convert specified columns to datetime\"\"\"\n",
    "    for col in columns:\n",
    "        df[col] = pd.to_datetime(df[col])\n",
    "    return df\n",
    "\n",
    "datetime_cols_lis = ['calendar_last_scraped', 'first_review', 'last_review', 'last_scraped', 'host_since']\n",
    "lis = convert_to_datetime(lis, datetime_cols_lis)\n",
    "cal['date'] = pd.to_datetime(cal['date'])\n",
    "rev['date'] = pd.to_datetime(rev['date'])\n",
    "\n",
    "\n",
    "def convert_to_type(df, columns, dtype):\n",
    "    \"\"\"Convert specified columns to given dtype\"\"\"\n",
    "    for col in columns:\n",
    "        df[col] = df[col].astype(dtype)\n",
    "    return df\n",
    "\n",
    "string_columns = ['bathrooms_text', 'neighbourhood', 'neighbourhood_cleansed', 'property_type', 'room_type', 'host_location', 'host_about', 'host_neighbourhood', 'listing_url', 'host_response_time', 'source', 'name','description','neighborhood_overview']\n",
    "lis = convert_to_type(lis, string_columns, \"string\")\n",
    "rev['comments'] = rev['comments'].astype(\"string\")\n",
    "\n",
    "\n",
    "percentage_cols = ['host_response_rate', 'host_acceptance_rate']\n",
    "for col in percentage_cols:\n",
    "    lis = lis.rename(columns={col: f\"{col}_pct\"})\n",
    "    lis[f\"{col}_pct\"] = lis[f\"{col}_pct\"].str.rstrip('%').astype('float') / 100\n",
    "\n",
    "# Currency inconsistency adjustment\n",
    "lis['price'] = lis['price'].str.replace(r'[\\$,]', '', regex=True)\n",
    "lis = lis.rename(columns={'price': 'price_DKK'})\n",
    "lis['price_DKK'] = pd.to_numeric(lis['price_DKK'], errors='coerce')\n",
    "\n",
    "cal['price'] = cal['price'].str.replace(r'[\\$,]', '', regex=True)\n",
    "cal = cal.rename(columns={'price': 'price_USD'})\n",
    "cal['price_USD'] = pd.to_numeric(cal['price_USD'], errors='coerce')\n",
    "\n",
    "########## Handling list columns ##########\n",
    "\n",
    "lis['amenities_count'] = lis.amenities.str.strip('[]').str.split(',').str.len()\n",
    "\n",
    "\n",
    "def clean_amenity(text):\n",
    "    \"\"\"Clean individual amenity strings\"\"\"\n",
    "    import re\n",
    "    text = str(text) # Convert to string if not already\n",
    "    text = text.strip().strip('\"\\'').strip('.- ') # Basic cleaning\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii') # Replace unicode escape sequences with their characters\n",
    "    text = re.sub(r'\\s+', ' ', text) # Replace multiple spaces with single space\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text) # Remove special characters, keeping only alphanumeric and spaces\n",
    "    text = text.lower().strip() # Convert to lowercase, strip again, and remove any remaining leading/trailing spaces\n",
    "    return text\n",
    "\n",
    "# Clean and convert amenities to comma-separated string\n",
    "lis['amenities'] = lis['amenities'].str.strip('[]').str.split(',').apply(\n",
    "    lambda x: ','.join(\n",
    "        sorted(  # Sort for consistency\n",
    "            filter(None,  # Remove empty strings\n",
    "                [clean_amenity(item) for item in x]\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "lis['amenities'] = lis['amenities'].astype('string') # Convert to string dtype\n",
    "\n",
    "# Count number of verifications per host\n",
    "lis['host_verifications_count'] = lis['host_verifications'].str.strip('[]').str.split(', ').str.len()\n",
    "lis['host_verifications'] = lis['host_verifications'].str.strip('[]').str.replace(\"'\", \"\").str.split(', ')\n",
    "# Create one-hot encoded columns\n",
    "verification_dummies = lis['host_verifications'].str.join('|').str.get_dummies()\n",
    "verification_dummies = verification_dummies.add_prefix('verification_')\n",
    "lis = pd.concat([lis, verification_dummies], axis=1)\n",
    "\n",
    "lis.drop(columns=['host_verifications', 'amenities'], inplace=True)\n",
    "\n",
    "print(\"Listings shape:\", lis.shape)\n",
    "print(\"Calendar shape:\", cal.shape) \n",
    "print(\"Reviews shape:\", rev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed datasets\n",
    "lis.to_parquet('data/processed/02_listings.parquet')\n",
    "cal.to_parquet('data/processed/02_calendar.parquet')\n",
    "rev.to_parquet('data/processed/02_reviews.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read to regular df, and maintain original Dtypes\n",
    "lis2 = pd.read_parquet('data/processed/02_listings.parquet')\n",
    "cal2 = pd.read_parquet('data/processed/02_calendar.parquet')\n",
    "rev2 = pd.read_parquet('data/processed/02_reviews.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using PostgreSQL COPY command\n",
    "# COPY table_name TO 'output.csv' WITH (FORMAT CSV, HEADER);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_df[lis_df['price_DKK'].isin(lis_df['price_DKK'].nlargest(5))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_df['price_DKK'].nlargest(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis.host_verifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis[lis.columns[-10:]]#.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis[lis.columns[-20:]].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis.iloc[:10, 40:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis[lis.columns[10:20]].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis[lis.columns[10:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data/processed/03_listings.parquet')\n",
    "verification_cols = [col for col in df.columns if col.startswith('verification_')]\n",
    "df[verification_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get verification columns and create comma-separated string\n",
    "verification_cols = [col.replace('verification_', '') for col in df.columns if col.startswith('verification_')]\n",
    "print(\"Available verification methods:\", ', '.join(verification_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_df['host_verifications']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate review score variance; consistency of the host's recieved reviews by guests\n",
    "# review_score_cols = [col for col in lis_df.columns if col.startswith('review_scores_')]\n",
    "# lis_df['review_scores_variance'] = lis_df[review_score_cols].var(axis=1)\n",
    "\n",
    "# Drop review score columns except rating and variance\n",
    "# cols_to_drop = [col for col in lis_df.columns if 'review_scores_' in col \n",
    "#                 and col != 'review_scores_rating' \n",
    "#                 and col != 'review_scores_variance']\n",
    "# lis_df.drop(columns=cols_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amenity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define key amenity categories that are most relevant for pricing/booking\n",
    "IMPORTANT_AMENITIES = {\n",
    "    'essentials': ['Wifi', 'Kitchen', 'Heating', 'Air conditioning', 'Washer'],\n",
    "    'luxury': ['Pool', 'Hot tub', 'Gym', 'Free parking'],\n",
    "    'safety': ['Smoke alarm', 'Carbon monoxide alarm', 'Fire extinguisher']\n",
    "}\n",
    "\n",
    "# Create binary columns for important amenities and category counts\n",
    "for category, items in IMPORTANT_AMENITIES.items():\n",
    "    # Create binary columns for each important amenity\n",
    "    for item in items:\n",
    "        lis[f'has_{item.lower().replace(\" \", \"_\")}'] = lis['amenities'].str.contains(item, case=False)\n",
    "    \n",
    "    # Create count for each category\n",
    "    lis[f'{category}_count'] = lis['amenities'].apply(\n",
    "        lambda x: sum(item.lower() in x.lower() for item in items)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed datasets\n",
    "lis.to_parquet('data/processed/02_listings.parquet')\n",
    "cal.to_parquet('data/processed/02_calendar.parquet')\n",
    "rev.to_parquet('data/processed/02_reviews.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read to regular df, and maintain original Dtypes\n",
    "lis2 = pd.read_parquet('data/processed/02_listings.parquet')\n",
    "cal2 = pd.read_parquet('data/processed/02_calendar.parquet')\n",
    "rev2 = pd.read_parquet('data/processed/02_reviews.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis['amenities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique amenities\n",
    "all_amenities = set()\n",
    "# lis['amenities'].str.split(',').apply(lambda x: [all_amenities.add(item.strip()) for item in x])\n",
    "lis['amenities'].str.strip('[]').str.split(',').apply(lambda x: [all_amenities.add(item.strip()) for item in x])\n",
    "\n",
    "# Sort and print the unique amenities to review\n",
    "sorted_amenities = sorted(all_amenities)\n",
    "\n",
    "print(f\"Total unique amenities: {len(sorted_amenities)}\")\n",
    "print(\"\\nAll unique amenities:\")\n",
    "for amenity in sorted_amenities:\n",
    "    print(f\"- {amenity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique amenities\n",
    "all_amenities = set()\n",
    "\n",
    "# Clean the string representation and split\n",
    "lis['amenities'].str.strip('[]').str.split(',').apply(\n",
    "    lambda x: [all_amenities.add(\n",
    "        # Clean each amenity string:\n",
    "        item.strip().strip('\"\\'').strip('.- ').lower()  # Remove quotes, dashes, dots, and extra spaces\n",
    "    ) for item in x]\n",
    ")\n",
    "\n",
    "# Sort and print the unique amenities to review\n",
    "sorted_amenities = sorted(all_amenities)\n",
    "print(f\"Total unique amenities: {len(sorted_amenities)}\")\n",
    "print(\"\\nAll unique amenities:\")\n",
    "for amenity in sorted_amenities:\n",
    "    print(f\"- {amenity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lis['amenities'].str.strip('[]').str.split(',').apply(\n",
    "#     lambda x: [all_amenities.add(\n",
    "#         # Clean each amenity string:\n",
    "#         item.strip().strip('\"\\'').strip('.- ').lower()  # Remove quotes, dashes, dots, and extra spaces\n",
    "#     ) for item in x])\n",
    "\n",
    "def clean_amenity(text):\n",
    "    \"\"\"Clean individual amenity strings\"\"\"\n",
    "    import re\n",
    "    text = str(text) # Convert to string if not already\n",
    "    text = text.strip().strip('\"\\'').strip('.- ') # Basic cleaning\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii') # Replace unicode escape sequences with their characters\n",
    "    text = re.sub(r'\\s+', ' ', text) # Replace multiple spaces with single space\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text) # Remove special characters, keeping only alphanumeric and spaces\n",
    "    text = text.lower().strip() # Convert to lowercase, strip again, and remove any remaining leading/trailing spaces\n",
    "    return text\n",
    "\n",
    "# Clean and convert amenities to comma-separated string\n",
    "lis['amenities'] = lis['amenities'].str.strip('[]').str.split(',').apply(\n",
    "    lambda x: ','.join(\n",
    "        sorted(  # Sort for consistency\n",
    "            filter(None,  # Remove empty strings\n",
    "                [clean_amenity(item) for item in x]\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "lis['amenities'] = lis['amenities'].astype('string') # Convert to string dtype\n",
    "\n",
    "# Get all unique amenities for sanity check\n",
    "all_amenities = set()\n",
    "lis['amenities'].str.strip('[]').str.split(',').apply(\n",
    "    lambda x: [all_amenities.add(clean_amenity(item)) for item in x if clean_amenity(item)]\n",
    ")\n",
    "all_amenities.discard('') # Remove empty strings if any made it through\n",
    "\n",
    "# Sort and print the unique amenities to review\n",
    "sorted_amenities = sorted(all_amenities)\n",
    "print(f\"Total unique amenities: {len(sorted_amenities)}\")\n",
    "print(\"\\nAll unique amenities:\")\n",
    "for amenity in sorted_amenities:\n",
    "    print(f\"- {amenity}\")\n",
    "\n",
    "# # Clean and convert amenities to comma-separated string\n",
    "# lis['amenities'] = lis['amenities'].str.strip('[]').str.split(',').apply(\n",
    "#     lambda x: ','.join(\n",
    "#         sorted(  # Sort for consistency\n",
    "#             filter(None, # Remove empty strings\n",
    "#                 [clean_amenity(item) for item in x]\n",
    "#             )\n",
    "#         )\n",
    "#     )\n",
    "# )\n",
    "\n",
    "\n",
    "# # Display a few examples of the cleaned amenities\n",
    "# print(\"Sample of cleaned amenities:\")\n",
    "# print(lis['amenities'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis['amenities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_amenities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lis.amenities.str.strip('[]').str.split(',').str.len().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cal.price.unique())#.nunique())\n",
    "lis.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking currency inconsistency in cal and lis csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal.loc[cal['listing_id'] == 262961]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis.loc[lis['id'] == 7631726, ['id', 'price_DKK']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis[['id','listing_url', 'price_DKK']][:2]\n",
    "# calendar_df[calendar_df['listing_id'] == 31094]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis = lis.where(pd.notnull(lis), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "# print(\"Calendar date range:\", cal['datetime'].min(), \"to\", cal['datetime'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get columns with less than 3 unique values\n",
    "low_unique_cols = [col for col in lis.columns if lis[col].nunique() < 3]\n",
    "print(\"Columns with less than 3 unique values:\")\n",
    "for col in low_unique_cols:\n",
    "    print(f\"{col}: {lis[col].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(rev.reviewer_name)#.nunique())\n",
    "rev.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01_init_EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_df = pd.read_parquet('data/processed/02_listings.parquet')\n",
    "cal_df = pd.read_parquet('data/processed/02_calendar.parquet')\n",
    "rev_df = pd.read_parquet('data/processed/02_reviews.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initial Data Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_dataset(df, name):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Dataset: {name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    print(\"\\n1. Basic Information:\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    \n",
    "    print(\"\\n2. Data Types:\")\n",
    "    print(df.dtypes)\n",
    "\n",
    "    print(\"\\n3. Summary Statistics:\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    print(\"\\n4. Unique Values:\")\n",
    "    for col in df.columns:\n",
    "        n_unique = df[col].nunique()\n",
    "        if n_unique < 3:\n",
    "            unique_values = df[col].unique()\n",
    "            print(f\"{col}: {n_unique} unique values\")\n",
    "            print(f\"Values: {unique_values}\\n\")\n",
    "\n",
    "    print(\"\\n5. Missing Values:\")\n",
    "    # Todo: Elias ad unique values + here\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df)) * 100\n",
    "    missing_info = pd.DataFrame({\n",
    "        'Missing Values': missing,\n",
    "        'Percentage': missing_pct\n",
    "    })\n",
    "    print(missing_info[missing_info['Missing Values'] > 0])\n",
    "    \n",
    "    return missing_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_missing = explore_dataset(lis_df, 'Listings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_missing = explore_dataset(cal_df, 'Calendar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_missing = explore_dataset(rev_df, 'Reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Missing Values Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_missing_values(missing_info, title):\n",
    "    # Filter columns with more than 0.1% missing values\n",
    "    missing_filtered = missing_info[missing_info['Percentage'] > 0.001]\n",
    "    \n",
    "    plt.figure(figsize=(18, 10))\n",
    "    ax = missing_filtered['Percentage'].plot(kind='bar')\n",
    "    plt.title(f'Missing Values in {title} Dataset')\n",
    "    plt.xlabel('Columns')\n",
    "    plt.ylabel('Percentage Missing')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    \n",
    "    # Add total missing values labels on top of each bar, rotated 45 degrees\n",
    "    for i, v in enumerate(missing_filtered['Missing Values']):\n",
    "        ax.text(i, missing_filtered['Percentage'].iloc[i], f'{int(v):,}', \n",
    "                ha='left', va='bottom', fontsize=8, rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot missing values for each dataset\n",
    "plot_missing_values(listings_missing, 'Listings')\n",
    "# plot_missing_values(calendar_missing, 'Calendar')\n",
    "# plot_missing_values(reviews_missing, 'Reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_data_quality_issues(df, name):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Data Quality Report for {name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # 1. Check for duplicates\n",
    "    n_duplicates = df.duplicated().sum()\n",
    "    if n_duplicates > 0:\n",
    "        print(f\"\\nDuplicate rows: {n_duplicates}\")\n",
    "    \n",
    "    # 2. Check for unexpected values\n",
    "    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    has_unexpected = False\n",
    "    for col in numeric_cols:\n",
    "        n_zeros = (df[col] == 0).sum()\n",
    "        n_negative = (df[col] < 0).sum()\n",
    "        if n_zeros > 0 or n_negative > 0:\n",
    "            if not has_unexpected:\n",
    "                print(\"\\nColumns with unexpected values:\")\n",
    "                has_unexpected = True\n",
    "            print(f\"\\n{col}:\")\n",
    "            if n_zeros > 0:\n",
    "                print(f\"- Zeros: {n_zeros} ({(n_zeros/len(df))*100:.2f}%)\")\n",
    "            if n_negative > 0:\n",
    "                print(f\"- Negative values: {n_negative} ({(n_negative/len(df))*100:.2f}%)\")\n",
    "    \n",
    "    # 3. Check string columns for data inconsistencies\n",
    "    string_cols = df.select_dtypes(include=['object']).columns\n",
    "    has_inconsistencies = False\n",
    "    for col in string_cols:\n",
    "        n_empty = (df[col] == '').sum()\n",
    "        n_whitespace = df[col].str.isspace().sum() if df[col].dtype == 'object' else 0\n",
    "        if n_empty > 0 or n_whitespace > 0:\n",
    "            if not has_inconsistencies:\n",
    "                print(\"\\nColumns with inconsistencies:\")\n",
    "                has_inconsistencies = True\n",
    "            print(f\"\\n{col}:\")\n",
    "            if n_empty > 0:\n",
    "                print(f\"- Empty strings: {n_empty}\")\n",
    "            if n_whitespace > 0:\n",
    "                print(f\"- Whitespace only: {n_whitespace}\")\n",
    "    \n",
    "    # 4. Check for extreme values in numeric columns\n",
    "    has_outliers = False\n",
    "    for col in numeric_cols:\n",
    "        mean = df[col].mean()\n",
    "        std = df[col].std()\n",
    "        outliers = df[col][abs(df[col] - mean) > 3*std]\n",
    "        if len(outliers) > 0:\n",
    "            if not has_outliers:\n",
    "                print(\"\\nColumns with outliers (beyond 3 std devs):\")\n",
    "                has_outliers = True\n",
    "            print(f\"\\n{col}:\")\n",
    "            print(f\"- Number of outliers: {len(outliers)}\")\n",
    "            print(f\"- Min outlier: {outliers.min()}\")\n",
    "            print(f\"- Max outlier: {outliers.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Listings dataset\n",
    "identify_data_quality_issues(lis_df, 'Listings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Calendar dataset\n",
    "identify_data_quality_issues(cal_df, 'Calendar')\n",
    "\n",
    "# Additional calendar-specific checks\n",
    "print(\"\\nChecking calendar date patterns:\")\n",
    "cal_df['date'] = pd.to_datetime(cal_df['date'])\n",
    "print(f\"Date range: {cal_df['date'].min()} to {cal_df['date'].max()}\")\n",
    "print(f\"Missing dates: {cal_df['date'].isnull().sum()}\")\n",
    "print(f\"Days between min and max date: {(cal_df['date'].max() - cal_df['date'].min()).days}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Reviews dataset\n",
    "print(identify_data_quality_issues(rev_df, 'Reviews'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Data Format Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_formats(df):\n",
    "    \"\"\"Check for inconsistent formats within columns\"\"\"\n",
    "    for col in df.columns:\n",
    "        # Get sample of unique values\n",
    "        unique_samples = df[col].dropna().unique()[:2]\n",
    "        print(f\"\\n{col}:\")\n",
    "        for sample in unique_samples:\n",
    "            print(f\"Value: {sample}, Type: {type(sample)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_data_formats(lis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_data_formats(cal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_data_formats(rev_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Special Characters Examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_special_characters(df):\n",
    "    \"\"\"Check for special characters that might need handling\"\"\"\n",
    "    string_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in string_cols:\n",
    "        # Fixed: Properly chain the .any() method\n",
    "        if df[col].astype(str).str.contains(r'[^a-zA-Z0-9\\s\\-.,:/+&æøåÆØÅ]').any():\n",
    "            print(f\"\\n{col} contains special characters\")\n",
    "            # Show examples of rows containing special characters\n",
    "            print(df[df[col].astype(str).str.contains(r'[^a-zA-Z0-9\\s\\-.,:/+&æøåÆØÅ]')][col].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_special_characters(lis_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_special_characters(cal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_special_characters(rev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
