{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listings shape: (20909, 75)\n",
      "Calendar shape: (7631731, 7)\n",
      "Reviews shape: (366636, 6)\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV files\n",
    "cal = pd.read_csv('data/raw/calendar2024.csv')\n",
    "lis = pd.read_csv('data/raw/listings2024.csv') \n",
    "rev = pd.read_csv('data/raw/reviews2024.csv')\n",
    "print(\"Listings shape:\", lis.shape)\n",
    "print(\"Calendar shape:\", cal.shape) \n",
    "print(\"Reviews shape:\", rev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31094</td>\n",
       "      <td>79346</td>\n",
       "      <td>2010-08-16</td>\n",
       "      <td>171607</td>\n",
       "      <td>Ben</td>\n",
       "      <td>We had a great stay. Conveniently located, qui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31094</td>\n",
       "      <td>166275</td>\n",
       "      <td>2011-01-05</td>\n",
       "      <td>306860</td>\n",
       "      <td>Makita</td>\n",
       "      <td>It was a very good stay. The appartment was re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31094</td>\n",
       "      <td>1452299</td>\n",
       "      <td>2012-06-10</td>\n",
       "      <td>1321058</td>\n",
       "      <td>Pierre</td>\n",
       "      <td>Really enjoyed my time at Ebbe's place.  It is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31094</td>\n",
       "      <td>6766430</td>\n",
       "      <td>2013-08-24</td>\n",
       "      <td>2182771</td>\n",
       "      <td>Sussie</td>\n",
       "      <td>The apartment was very well located, 10-15 min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31094</td>\n",
       "      <td>6827217</td>\n",
       "      <td>2013-08-26</td>\n",
       "      <td>8025926</td>\n",
       "      <td>Wil</td>\n",
       "      <td>This is a great flat, very clean with everythi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   listing_id       id        date  reviewer_id reviewer_name  \\\n",
       "0       31094    79346  2010-08-16       171607           Ben   \n",
       "1       31094   166275  2011-01-05       306860        Makita   \n",
       "2       31094  1452299  2012-06-10      1321058        Pierre   \n",
       "3       31094  6766430  2013-08-24      2182771        Sussie   \n",
       "4       31094  6827217  2013-08-26      8025926           Wil   \n",
       "\n",
       "                                            comments  \n",
       "0  We had a great stay. Conveniently located, qui...  \n",
       "1  It was a very good stay. The appartment was re...  \n",
       "2  Really enjoyed my time at Ebbe's place.  It is...  \n",
       "3  The apartment was very well located, 10-15 min...  \n",
       "4  This is a great flat, very clean with everythi...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20909 entries, 0 to 20908\n",
      "Data columns (total 75 columns):\n",
      " #   Column                                        Non-Null Count  Dtype  \n",
      "---  ------                                        --------------  -----  \n",
      " 0   id                                            20909 non-null  int64  \n",
      " 1   listing_url                                   20909 non-null  object \n",
      " 2   scrape_id                                     20909 non-null  int64  \n",
      " 3   last_scraped                                  20909 non-null  object \n",
      " 4   source                                        20909 non-null  object \n",
      " 5   name                                          20909 non-null  object \n",
      " 6   description                                   20235 non-null  object \n",
      " 7   neighborhood_overview                         8984 non-null   object \n",
      " 8   picture_url                                   20909 non-null  object \n",
      " 9   host_id                                       20909 non-null  int64  \n",
      " 10  host_url                                      20909 non-null  object \n",
      " 11  host_name                                     20908 non-null  object \n",
      " 12  host_since                                    20908 non-null  object \n",
      " 13  host_location                                 17480 non-null  object \n",
      " 14  host_about                                    8866 non-null   object \n",
      " 15  host_response_time                            14441 non-null  object \n",
      " 16  host_response_rate                            14441 non-null  object \n",
      " 17  host_acceptance_rate                          17161 non-null  object \n",
      " 18  host_is_superhost                             20685 non-null  object \n",
      " 19  host_thumbnail_url                            20908 non-null  object \n",
      " 20  host_picture_url                              20908 non-null  object \n",
      " 21  host_neighbourhood                            5417 non-null   object \n",
      " 22  host_listings_count                           20908 non-null  float64\n",
      " 23  host_total_listings_count                     20908 non-null  float64\n",
      " 24  host_verifications                            20908 non-null  object \n",
      " 25  host_has_profile_pic                          20908 non-null  object \n",
      " 26  host_identity_verified                        20908 non-null  object \n",
      " 27  neighbourhood                                 8984 non-null   object \n",
      " 28  neighbourhood_cleansed                        20909 non-null  object \n",
      " 29  neighbourhood_group_cleansed                  0 non-null      float64\n",
      " 30  latitude                                      20909 non-null  float64\n",
      " 31  longitude                                     20909 non-null  float64\n",
      " 32  property_type                                 20909 non-null  object \n",
      " 33  room_type                                     20909 non-null  object \n",
      " 34  accommodates                                  20909 non-null  int64  \n",
      " 35  bathrooms                                     13660 non-null  float64\n",
      " 36  bathrooms_text                                20903 non-null  object \n",
      " 37  bedrooms                                      20300 non-null  float64\n",
      " 38  beds                                          13660 non-null  float64\n",
      " 39  amenities                                     20909 non-null  object \n",
      " 40  price                                         13596 non-null  object \n",
      " 41  minimum_nights                                20909 non-null  int64  \n",
      " 42  maximum_nights                                20909 non-null  int64  \n",
      " 43  minimum_minimum_nights                        20909 non-null  int64  \n",
      " 44  maximum_minimum_nights                        20909 non-null  int64  \n",
      " 45  minimum_maximum_nights                        20909 non-null  int64  \n",
      " 46  maximum_maximum_nights                        20909 non-null  int64  \n",
      " 47  minimum_nights_avg_ntm                        20909 non-null  float64\n",
      " 48  maximum_nights_avg_ntm                        20909 non-null  float64\n",
      " 49  calendar_updated                              0 non-null      float64\n",
      " 50  has_availability                              20437 non-null  object \n",
      " 51  availability_30                               20909 non-null  int64  \n",
      " 52  availability_60                               20909 non-null  int64  \n",
      " 53  availability_90                               20909 non-null  int64  \n",
      " 54  availability_365                              20909 non-null  int64  \n",
      " 55  calendar_last_scraped                         20909 non-null  object \n",
      " 56  number_of_reviews                             20909 non-null  int64  \n",
      " 57  number_of_reviews_ltm                         20909 non-null  int64  \n",
      " 58  number_of_reviews_l30d                        20909 non-null  int64  \n",
      " 59  first_review                                  17689 non-null  object \n",
      " 60  last_review                                   17689 non-null  object \n",
      " 61  review_scores_rating                          17689 non-null  float64\n",
      " 62  review_scores_accuracy                        17665 non-null  float64\n",
      " 63  review_scores_cleanliness                     17665 non-null  float64\n",
      " 64  review_scores_checkin                         17665 non-null  float64\n",
      " 65  review_scores_communication                   17665 non-null  float64\n",
      " 66  review_scores_location                        17664 non-null  float64\n",
      " 67  review_scores_value                           17664 non-null  float64\n",
      " 68  license                                       0 non-null      float64\n",
      " 69  instant_bookable                              20909 non-null  object \n",
      " 70  calculated_host_listings_count                20909 non-null  int64  \n",
      " 71  calculated_host_listings_count_entire_homes   20909 non-null  int64  \n",
      " 72  calculated_host_listings_count_private_rooms  20909 non-null  int64  \n",
      " 73  calculated_host_listings_count_shared_rooms   20909 non-null  int64  \n",
      " 74  reviews_per_month                             17689 non-null  float64\n",
      "dtypes: float64(20), int64(21), object(34)\n",
      "memory usage: 12.0+ MB\n"
     ]
    }
   ],
   "source": [
    "### Remap scale back in sentiment analysis\n",
    "// ... existing code ...\n",
    "\n",
    "# Load the sentiment scores\n",
    "df = pd.read_parquet('data/processed/04_sentiment_bert.parquet')\n",
    "\n",
    "# Convert sentiment scores back to 1-5 scale\n",
    "df['sentiment_score_1_5'] = (df['sentiment_score'] * 2) + 3\n",
    "\n",
    "# Save the updated dataframe\n",
    "df.to_parquet('data/processed/04_sentiment_bert.parquet')\n",
    "// ... existing code ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "        n_unique = df[col].nunique()\n",
    "        if n_unique < 3:\n",
    "            unique_values = df[col].unique()\n",
    "            print(f\"{col}: {n_unique} unique values\")\n",
    "            print(f\"Values: {unique_values}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_cols_lis = lis.columns[lis.isna().all()].tolist()\n",
    "lis = lis.drop(columns=null_cols_lis)\n",
    "\n",
    "lis.drop(columns=['scrape_id', 'host_name', 'picture_url', 'host_url', 'host_thumbnail_url', 'host_picture_url'], inplace=True)\n",
    "cal.drop(columns=['adjusted_price'], inplace=True)\n",
    "rev.drop(columns=['reviewer_name'], inplace=True)\n",
    "\n",
    "\n",
    "def convert_to_boolean(df, columns, true_value='t'):\n",
    "    \"\"\"Convert specified columns from string indicators to boolean\"\"\"\n",
    "    for col in columns:\n",
    "        df[col] = df[col] == true_value\n",
    "    return df\n",
    "\n",
    "boolean_cols = ['instant_bookable', 'host_is_superhost', 'host_has_profile_pic', 'host_identity_verified', 'has_availability']\n",
    "lis = convert_to_boolean(lis, boolean_cols)\n",
    "cal['available'] = cal['available'] == 't'\n",
    "\n",
    "\n",
    "def convert_to_datetime(df, columns):\n",
    "    \"\"\"Convert specified columns to datetime\"\"\"\n",
    "    for col in columns:\n",
    "        df[col] = pd.to_datetime(df[col])\n",
    "    return df\n",
    "\n",
    "datetime_cols_lis = ['calendar_last_scraped', 'first_review', 'last_review', 'last_scraped', 'host_since']\n",
    "lis = convert_to_datetime(lis, datetime_cols_lis)\n",
    "cal['date'] = pd.to_datetime(cal['date'])\n",
    "rev['date'] = pd.to_datetime(rev['date'])\n",
    "\n",
    "\n",
    "def convert_to_type(df, columns, dtype):\n",
    "    \"\"\"Convert specified columns to given dtype\"\"\"\n",
    "    for col in columns:\n",
    "        df[col] = df[col].astype(dtype)\n",
    "    return df\n",
    "\n",
    "string_columns = ['bathrooms_text', 'neighbourhood', 'neighbourhood_cleansed', 'property_type', 'room_type', 'host_location', 'host_about', 'host_neighbourhood', 'listing_url', 'host_response_time', 'source', 'name','description','neighborhood_overview']\n",
    "lis = convert_to_type(lis, string_columns, \"string\")\n",
    "rev['comments'] = rev['comments'].astype(\"string\")\n",
    "\n",
    "\n",
    "percentage_cols = ['host_response_rate', 'host_acceptance_rate']\n",
    "for col in percentage_cols:\n",
    "    lis = lis.rename(columns={col: f\"{col}_pct\"})\n",
    "    lis[f\"{col}_pct\"] = lis[f\"{col}_pct\"].str.rstrip('%').astype('float') / 100\n",
    "\n",
    "# Currency inconsistency adjustment\n",
    "lis['price'] = lis['price'].str.replace(r'[\\$,]', '', regex=True)\n",
    "lis = lis.rename(columns={'price': 'price_DKK'})\n",
    "lis['price_DKK'] = pd.to_numeric(lis['price_DKK'], errors='coerce')\n",
    "\n",
    "cal['price'] = cal['price'].str.replace(r'[\\$,]', '', regex=True)\n",
    "cal = cal.rename(columns={'price': 'price_USD'})\n",
    "cal['price_USD'] = pd.to_numeric(cal['price_USD'], errors='coerce')\n",
    "\n",
    "########## Handling list columns ##########\n",
    "\n",
    "lis['amenities_count'] = lis.amenities.str.strip('[]').str.split(',').str.len()\n",
    "\n",
    "\n",
    "def clean_amenity(text):\n",
    "    \"\"\"Clean individual amenity strings\"\"\"\n",
    "    import re\n",
    "    text = str(text) # Convert to string if not already\n",
    "    text = text.strip().strip('\"\\'').strip('.- ') # Basic cleaning\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii') # Replace unicode escape sequences with their characters\n",
    "    text = re.sub(r'\\s+', ' ', text) # Replace multiple spaces with single space\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text) # Remove special characters, keeping only alphanumeric and spaces\n",
    "    text = text.lower().strip() # Convert to lowercase, strip again, and remove any remaining leading/trailing spaces\n",
    "    return text\n",
    "\n",
    "# Clean and convert amenities to comma-separated string\n",
    "lis['amenities'] = lis['amenities'].str.strip('[]').str.split(',').apply(\n",
    "    lambda x: ','.join(\n",
    "        sorted(  # Sort for consistency\n",
    "            filter(None,  # Remove empty strings\n",
    "                [clean_amenity(item) for item in x]\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "lis['amenities'] = lis['amenities'].astype('string') # Convert to string dtype\n",
    "\n",
    "# Count number of verifications per host\n",
    "lis['host_verifications_count'] = lis['host_verifications'].str.strip('[]').str.split(', ').str.len()\n",
    "lis['host_verifications'] = lis['host_verifications'].str.strip('[]').str.replace(\"'\", \"\").str.split(', ')\n",
    "# Create one-hot encoded columns\n",
    "verification_dummies = lis['host_verifications'].str.join('|').str.get_dummies()\n",
    "verification_dummies = verification_dummies.add_prefix('verification_')\n",
    "lis = pd.concat([lis, verification_dummies], axis=1)\n",
    "\n",
    "lis.drop(columns=['host_verifications', 'amenities'], inplace=True)\n",
    "\n",
    "print(\"Listings shape:\", lis.shape)\n",
    "print(\"Calendar shape:\", cal.shape) \n",
    "print(\"Reviews shape:\", rev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed datasets\n",
    "lis.to_parquet('data/processed/02_listings.parquet')\n",
    "cal.to_parquet('data/processed/02_calendar.parquet')\n",
    "rev.to_parquet('data/processed/02_reviews.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read to regular df, and maintain original Dtypes\n",
    "lis2 = pd.read_parquet('data/processed/02_listings.parquet')\n",
    "cal2 = pd.read_parquet('data/processed/02_calendar.parquet')\n",
    "rev2 = pd.read_parquet('data/processed/02_reviews.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using PostgreSQL COPY command\n",
    "# COPY table_name TO 'output.csv' WITH (FORMAT CSV, HEADER);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_df[lis_df['price_DKK'].isin(lis_df['price_DKK'].nlargest(5))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_df['price_DKK'].nlargest(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis.host_verifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis[lis.columns[-10:]]#.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis[lis.columns[-20:]].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis.iloc[:10, 40:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis[lis.columns[10:20]].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis[lis.columns[10:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert parquet to CSV\n",
    "pd.read_parquet('data/processed/03_calendar.parquet').to_csv('data/processed/03_calendar.csv', index=False)\n",
    "pd.read_parquet('data/processed/03_listings.parquet').to_csv('data/processed/03_listings.csv', index=False)\n",
    "pd.read_parquet('data/processed/03_reviews.parquet').to_csv('data/processed/03_reviews.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data/processed/03_listings.parquet')\n",
    "verification_cols = [col for col in df.columns if col.startswith('verification_')]\n",
    "df[verification_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get verification columns and create comma-separated string\n",
    "verification_cols = [col.replace('verification_', '') for col in df.columns if col.startswith('verification_')]\n",
    "print(\"Available verification methods:\", ', '.join(verification_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_df['host_verifications']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amenity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define key amenity categories that are most relevant for pricing/booking\n",
    "IMPORTANT_AMENITIES = {\n",
    "    'essentials': ['Wifi', 'Kitchen', 'Heating', 'Air conditioning', 'Washer'],\n",
    "    'luxury': ['Pool', 'Hot tub', 'Gym', 'Free parking'],\n",
    "    'safety': ['Smoke alarm', 'Carbon monoxide alarm', 'Fire extinguisher']\n",
    "}\n",
    "\n",
    "# Create binary columns for important amenities and category counts\n",
    "for category, items in IMPORTANT_AMENITIES.items():\n",
    "    # Create binary columns for each important amenity\n",
    "    for item in items:\n",
    "        lis[f'has_{item.lower().replace(\" \", \"_\")}'] = lis['amenities'].str.contains(item, case=False)\n",
    "    \n",
    "    # Create count for each category\n",
    "    lis[f'{category}_count'] = lis['amenities'].apply(\n",
    "        lambda x: sum(item.lower() in x.lower() for item in items)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed datasets\n",
    "lis.to_parquet('data/processed/02_listings.parquet')\n",
    "cal.to_parquet('data/processed/02_calendar.parquet')\n",
    "rev.to_parquet('data/processed/02_reviews.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read to regular df, and maintain original Dtypes\n",
    "lis2 = pd.read_parquet('data/processed/02_listings.parquet')\n",
    "cal2 = pd.read_parquet('data/processed/02_calendar.parquet')\n",
    "rev2 = pd.read_parquet('data/processed/02_reviews.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis['amenities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique amenities\n",
    "all_amenities = set()\n",
    "# lis['amenities'].str.split(',').apply(lambda x: [all_amenities.add(item.strip()) for item in x])\n",
    "lis['amenities'].str.strip('[]').str.split(',').apply(lambda x: [all_amenities.add(item.strip()) for item in x])\n",
    "\n",
    "# Sort and print the unique amenities to review\n",
    "sorted_amenities = sorted(all_amenities)\n",
    "\n",
    "print(f\"Total unique amenities: {len(sorted_amenities)}\")\n",
    "print(\"\\nAll unique amenities:\")\n",
    "for amenity in sorted_amenities:\n",
    "    print(f\"- {amenity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique amenities\n",
    "all_amenities = set()\n",
    "\n",
    "# Clean the string representation and split\n",
    "lis['amenities'].str.strip('[]').str.split(',').apply(\n",
    "    lambda x: [all_amenities.add(\n",
    "        # Clean each amenity string:\n",
    "        item.strip().strip('\"\\'').strip('.- ').lower()  # Remove quotes, dashes, dots, and extra spaces\n",
    "    ) for item in x]\n",
    ")\n",
    "\n",
    "# Sort and print the unique amenities to review\n",
    "sorted_amenities = sorted(all_amenities)\n",
    "print(f\"Total unique amenities: {len(sorted_amenities)}\")\n",
    "print(\"\\nAll unique amenities:\")\n",
    "for amenity in sorted_amenities:\n",
    "    print(f\"- {amenity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lis['amenities'].str.strip('[]').str.split(',').apply(\n",
    "#     lambda x: [all_amenities.add(\n",
    "#         # Clean each amenity string:\n",
    "#         item.strip().strip('\"\\'').strip('.- ').lower()  # Remove quotes, dashes, dots, and extra spaces\n",
    "#     ) for item in x])\n",
    "\n",
    "def clean_amenity(text):\n",
    "    \"\"\"Clean individual amenity strings\"\"\"\n",
    "    import re\n",
    "    text = str(text) # Convert to string if not already\n",
    "    text = text.strip().strip('\"\\'').strip('.- ') # Basic cleaning\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii') # Replace unicode escape sequences with their characters\n",
    "    text = re.sub(r'\\s+', ' ', text) # Replace multiple spaces with single space\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text) # Remove special characters, keeping only alphanumeric and spaces\n",
    "    text = text.lower().strip() # Convert to lowercase, strip again, and remove any remaining leading/trailing spaces\n",
    "    return text\n",
    "\n",
    "# Clean and convert amenities to comma-separated string\n",
    "lis['amenities'] = lis['amenities'].str.strip('[]').str.split(',').apply(\n",
    "    lambda x: ','.join(\n",
    "        sorted(  # Sort for consistency\n",
    "            filter(None,  # Remove empty strings\n",
    "                [clean_amenity(item) for item in x]\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "lis['amenities'] = lis['amenities'].astype('string') # Convert to string dtype\n",
    "\n",
    "# Get all unique amenities for sanity check\n",
    "all_amenities = set()\n",
    "lis['amenities'].str.strip('[]').str.split(',').apply(\n",
    "    lambda x: [all_amenities.add(clean_amenity(item)) for item in x if clean_amenity(item)]\n",
    ")\n",
    "all_amenities.discard('') # Remove empty strings if any made it through\n",
    "\n",
    "# Sort and print the unique amenities to review\n",
    "sorted_amenities = sorted(all_amenities)\n",
    "print(f\"Total unique amenities: {len(sorted_amenities)}\")\n",
    "print(\"\\nAll unique amenities:\")\n",
    "for amenity in sorted_amenities:\n",
    "    print(f\"- {amenity}\")\n",
    "\n",
    "# # Clean and convert amenities to comma-separated string\n",
    "# lis['amenities'] = lis['amenities'].str.strip('[]').str.split(',').apply(\n",
    "#     lambda x: ','.join(\n",
    "#         sorted(  # Sort for consistency\n",
    "#             filter(None, # Remove empty strings\n",
    "#                 [clean_amenity(item) for item in x]\n",
    "#             )\n",
    "#         )\n",
    "#     )\n",
    "# )\n",
    "\n",
    "\n",
    "# # Display a few examples of the cleaned amenities\n",
    "# print(\"Sample of cleaned amenities:\")\n",
    "# print(lis['amenities'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis['amenities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_amenities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lis.amenities.str.strip('[]').str.split(',').str.len().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cal.price.unique())#.nunique())\n",
    "lis.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking currency inconsistency in cal and lis csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal.loc[cal['listing_id'] == 262961]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis.loc[lis['id'] == 7631726, ['id', 'price_DKK']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis[['id','listing_url', 'price_DKK']][:2]\n",
    "# calendar_df[calendar_df['listing_id'] == 31094]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis = lis.where(pd.notnull(lis), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "# print(\"Calendar date range:\", cal['datetime'].min(), \"to\", cal['datetime'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get columns with less than 3 unique values\n",
    "low_unique_cols = [col for col in lis.columns if lis[col].nunique() < 3]\n",
    "print(\"Columns with less than 3 unique values:\")\n",
    "for col in low_unique_cols:\n",
    "    print(f\"{col}: {lis[col].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(rev.reviewer_name)#.nunique())\n",
    "rev.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01_init_EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_df = pd.read_parquet('data/processed/02_listings.parquet')\n",
    "cal_df = pd.read_parquet('data/processed/02_calendar.parquet')\n",
    "rev_df = pd.read_parquet('data/processed/02_reviews.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initial Data Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_dataset(df, name):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Dataset: {name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    print(\"\\n1. Basic Information:\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    \n",
    "    print(\"\\n2. Data Types:\")\n",
    "    print(df.dtypes)\n",
    "\n",
    "    print(\"\\n3. Summary Statistics:\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    print(\"\\n4. Unique Values:\")\n",
    "    for col in df.columns:\n",
    "        n_unique = df[col].nunique()\n",
    "        if n_unique < 3:\n",
    "            unique_values = df[col].unique()\n",
    "            print(f\"{col}: {n_unique} unique values\")\n",
    "            print(f\"Values: {unique_values}\\n\")\n",
    "\n",
    "    print(\"\\n5. Missing Values:\")\n",
    "    # Todo: Elias ad unique values + here\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df)) * 100\n",
    "    missing_info = pd.DataFrame({\n",
    "        'Missing Values': missing,\n",
    "        'Percentage': missing_pct\n",
    "    })\n",
    "    print(missing_info[missing_info['Missing Values'] > 0])\n",
    "    \n",
    "    return missing_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_missing = explore_dataset(lis_df, 'Listings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_missing = explore_dataset(cal_df, 'Calendar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_missing = explore_dataset(rev_df, 'Reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Missing Values Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_missing_values(missing_info, title):\n",
    "    # Filter columns with more than 0.1% missing values\n",
    "    missing_filtered = missing_info[missing_info['Percentage'] > 0.001]\n",
    "    \n",
    "    plt.figure(figsize=(18, 10))\n",
    "    ax = missing_filtered['Percentage'].plot(kind='bar')\n",
    "    plt.title(f'Missing Values in {title} Dataset')\n",
    "    plt.xlabel('Columns')\n",
    "    plt.ylabel('Percentage Missing')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    \n",
    "    # Add total missing values labels on top of each bar, rotated 45 degrees\n",
    "    for i, v in enumerate(missing_filtered['Missing Values']):\n",
    "        ax.text(i, missing_filtered['Percentage'].iloc[i], f'{int(v):,}', \n",
    "                ha='left', va='bottom', fontsize=8, rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot missing values for each dataset\n",
    "plot_missing_values(listings_missing, 'Listings')\n",
    "# plot_missing_values(calendar_missing, 'Calendar')\n",
    "# plot_missing_values(reviews_missing, 'Reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_data_quality_issues(df, name):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Data Quality Report for {name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # 1. Check for duplicates\n",
    "    n_duplicates = df.duplicated().sum()\n",
    "    if n_duplicates > 0:\n",
    "        print(f\"\\nDuplicate rows: {n_duplicates}\")\n",
    "    \n",
    "    # 2. Check for unexpected values\n",
    "    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    has_unexpected = False\n",
    "    for col in numeric_cols:\n",
    "        n_zeros = (df[col] == 0).sum()\n",
    "        n_negative = (df[col] < 0).sum()\n",
    "        if n_zeros > 0 or n_negative > 0:\n",
    "            if not has_unexpected:\n",
    "                print(\"\\nColumns with unexpected values:\")\n",
    "                has_unexpected = True\n",
    "            print(f\"\\n{col}:\")\n",
    "            if n_zeros > 0:\n",
    "                print(f\"- Zeros: {n_zeros} ({(n_zeros/len(df))*100:.2f}%)\")\n",
    "            if n_negative > 0:\n",
    "                print(f\"- Negative values: {n_negative} ({(n_negative/len(df))*100:.2f}%)\")\n",
    "    \n",
    "    # 3. Check string columns for data inconsistencies\n",
    "    string_cols = df.select_dtypes(include=['object']).columns\n",
    "    has_inconsistencies = False\n",
    "    for col in string_cols:\n",
    "        n_empty = (df[col] == '').sum()\n",
    "        n_whitespace = df[col].str.isspace().sum() if df[col].dtype == 'object' else 0\n",
    "        if n_empty > 0 or n_whitespace > 0:\n",
    "            if not has_inconsistencies:\n",
    "                print(\"\\nColumns with inconsistencies:\")\n",
    "                has_inconsistencies = True\n",
    "            print(f\"\\n{col}:\")\n",
    "            if n_empty > 0:\n",
    "                print(f\"- Empty strings: {n_empty}\")\n",
    "            if n_whitespace > 0:\n",
    "                print(f\"- Whitespace only: {n_whitespace}\")\n",
    "    \n",
    "    # 4. Check for extreme values in numeric columns\n",
    "    has_outliers = False\n",
    "    for col in numeric_cols:\n",
    "        mean = df[col].mean()\n",
    "        std = df[col].std()\n",
    "        outliers = df[col][abs(df[col] - mean) > 3*std]\n",
    "        if len(outliers) > 0:\n",
    "            if not has_outliers:\n",
    "                print(\"\\nColumns with outliers (beyond 3 std devs):\")\n",
    "                has_outliers = True\n",
    "            print(f\"\\n{col}:\")\n",
    "            print(f\"- Number of outliers: {len(outliers)}\")\n",
    "            print(f\"- Min outlier: {outliers.min()}\")\n",
    "            print(f\"- Max outlier: {outliers.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Listings dataset\n",
    "identify_data_quality_issues(lis_df, 'Listings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Calendar dataset\n",
    "identify_data_quality_issues(cal_df, 'Calendar')\n",
    "\n",
    "# Additional calendar-specific checks\n",
    "print(\"\\nChecking calendar date patterns:\")\n",
    "cal_df['date'] = pd.to_datetime(cal_df['date'])\n",
    "print(f\"Date range: {cal_df['date'].min()} to {cal_df['date'].max()}\")\n",
    "print(f\"Missing dates: {cal_df['date'].isnull().sum()}\")\n",
    "print(f\"Days between min and max date: {(cal_df['date'].max() - cal_df['date'].min()).days}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Reviews dataset\n",
    "print(identify_data_quality_issues(rev_df, 'Reviews'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Data Format Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_formats(df):\n",
    "    \"\"\"Check for inconsistent formats within columns\"\"\"\n",
    "    for col in df.columns:\n",
    "        # Get sample of unique values\n",
    "        unique_samples = df[col].dropna().unique()[:2]\n",
    "        print(f\"\\n{col}:\")\n",
    "        for sample in unique_samples:\n",
    "            print(f\"Value: {sample}, Type: {type(sample)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_data_formats(lis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_data_formats(cal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_data_formats(rev_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Special Characters Examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_special_characters(df):\n",
    "    \"\"\"Check for special characters that might need handling\"\"\"\n",
    "    string_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in string_cols:\n",
    "        # Fixed: Properly chain the .any() method\n",
    "        if df[col].astype(str).str.contains(r'[^a-zA-Z0-9\\s\\-.,:/+&æøåÆØÅ]').any():\n",
    "            print(f\"\\n{col} contains special characters\")\n",
    "            # Show examples of rows containing special characters\n",
    "            print(df[df[col].astype(str).str.contains(r'[^a-zA-Z0-9\\s\\-.,:/+&æøåÆØÅ]')][col].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_special_characters(lis_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_special_characters(cal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_special_characters(rev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
