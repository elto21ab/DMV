{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_df = pd.read_csv('data/raw/calendar2024.csv')\n",
    "lis_df = pd.read_csv('data/raw/listings2024.csv') \n",
    "rev_df = pd.read_csv('data/raw/reviews2024.csv')\n",
    "print('rev:',rev_df.shape)\n",
    "print('lis:',lis_df.shape)\n",
    "print('cal:',cal_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initial Column Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty columns\n",
    "null_cols_lis = lis_df.columns[lis_df.isna().all()].tolist()\n",
    "lis_df = lis_df.drop(columns=null_cols_lis)\n",
    "\n",
    "# Drop completely irrelevant columns from listings data\n",
    "lis_df.drop(columns=['scrape_id', 'host_name', 'picture_url', 'host_url', 'host_thumbnail_url', \n",
    "                     'host_picture_url', 'host_listings_count', 'calculated_host_listings_count', \n",
    "                     'calculated_host_listings_count_entire_homes', 'calculated_host_listings_count_private_rooms', \n",
    "                     'calculated_host_listings_count_shared_rooms',\n",
    "                     'number_of_reviews_l30d', 'number_of_reviews_ltm', \n",
    "                     'last_scraped', 'source', 'calendar_last_scraped',\n",
    "                     'host_about', 'neighborhood_overview',\n",
    "                     ], inplace=True) \n",
    "\n",
    "lis_df.drop(columns=[col for col in lis_df.columns if 'host_listings' in col], inplace=True)\n",
    "lis_df.drop(columns=[col for col in lis_df.columns if 'availability_' in col], inplace=True)\n",
    "lis_df.drop(columns=[col for col in lis_df.columns if 'minimum_nights' in col], inplace=True)\n",
    "lis_df.drop(columns=[col for col in lis_df.columns if 'maximum_nights' in col], inplace=True)\n",
    "\n",
    "rev_df.drop(columns=['reviewer_name'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename\n",
    "lis_df['superhost'] = lis_df['host_is_superhost']\n",
    "lis_df.drop(['host_is_superhost'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neigborhood Attribute Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    'Nrrebro': 'Nørrebro',\n",
    "    'sterbro': 'Østerbro',\n",
    "    'Vanlse': 'Vanløse',\n",
    "    'Brnshj-Husum': 'Brønshøj',\n",
    "    'Vesterbro-Kongens Enghave': 'Vesterbro',\n",
    "}\n",
    "\n",
    "lis_df['neighbourhood_cleansed'] = lis_df['neighbourhood_cleansed'].replace(mapping)\n",
    "\n",
    "\n",
    "# Replace empty or NA host_neighbourhood values with corresponding neighbourhood_cleansed values\n",
    "mask = (lis_df['host_neighbourhood'] == '') | (lis_df['host_neighbourhood'].isna())\n",
    "lis_df.loc[mask, 'host_neighbourhood'] = lis_df.loc[mask, 'neighbourhood_cleansed']\n",
    "\n",
    "#########################################################\n",
    "\n",
    "non_cph_neighborhoods = ['', 'Nyboder', 'Niagara', \n",
    "                        '6th Arrondissement', 'Södermalm',\n",
    "                         'Embajadores', 'Batignolles', 'Montmartre', 'Vesturbær', 'Passy',\n",
    "                         'Almagro', 'Clinton Hill', 'Jakkur Layout', 'Jardim das Bandeiras',\n",
    "                         'Williamsburg', 'Notting Hill', 'Belém', 'Campo Belo', 'El Madroñal',\n",
    "                         'Alphabet City', 'Buzovna', 'Bastille']\n",
    "\n",
    "# Count listings for each non-Copenhagen neighborhood\n",
    "non_cph_counts = lis_df[lis_df['host_neighbourhood'].isin(non_cph_neighborhoods)]['host_neighbourhood'].value_counts()\n",
    "\n",
    "# Get neighborhoods with count < 9 from non_cph_counts\n",
    "small_non_cph = non_cph_counts[non_cph_counts < 9].index\n",
    "\n",
    "# Update host_neighbourhood to match neighbourhood_cleansed where host_neighbourhood is in small_non_cph\n",
    "mask = lis_df['host_neighbourhood'].isin(small_non_cph)\n",
    "lis_df.loc[mask, 'host_neighbourhood'] = lis_df.loc[mask, 'neighbourhood_cleansed']\n",
    "\n",
    "# Remap Hackney to Nørrebro\n",
    "lis_df.loc[lis_df['host_neighbourhood'] == 'Hackney', 'host_neighbourhood'] = 'Nørrebro'\n",
    "\n",
    "lis_df = lis_df.drop(['neighbourhood', 'host_location'], axis=1) # 'neighbourhood_cleansed' is just a simplified host_neighbourhood with less unique values\n",
    "print(lis_df['host_neighbourhood'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Type Conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def convert_to_boolean(df, columns, true_value='t'):\n",
    "    \"\"\"Convert specified columns from string indicators to boolean\"\"\"\n",
    "    for col in columns:\n",
    "        df[col] = df[col] == true_value\n",
    "    return df\n",
    "\n",
    "def convert_to_datetime(df, columns):\n",
    "    \"\"\"Convert specified columns to datetime\"\"\"\n",
    "    for col in columns:\n",
    "        df[col] = pd.to_datetime(df[col])\n",
    "    return df\n",
    "\n",
    "def convert_to_type(df, columns, dtype):\n",
    "    \"\"\"Convert specified columns to given dtype\"\"\"\n",
    "    for col in columns:\n",
    "        df[col] = df[col].astype(dtype)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean conversions\n",
    "boolean_cols = ['instant_bookable', 'host_has_profile_pic', 'host_identity_verified', 'has_availability', 'superhost']\n",
    "lis_df = convert_to_boolean(lis_df, boolean_cols)\n",
    "# cal_df['available'] = cal_df['available'] == 't'\n",
    "\n",
    "# Datetime conversions\n",
    "datetime_cols_lis = ['first_review', 'last_review', 'host_since']\n",
    "lis_df = convert_to_datetime(lis_df, datetime_cols_lis)\n",
    "# cal_df['date'] = pd.to_datetime(cal_df['date'])\n",
    "rev_df['date'] = pd.to_datetime(rev_df['date'])\n",
    "\n",
    "# String conversions\n",
    "string_columns = ['bathrooms_text', 'neighbourhood_cleansed', 'property_type', 'room_type', 'host_neighbourhood', 'listing_url', 'host_response_time', 'name', 'description']\n",
    "lis_df = convert_to_type(lis_df, string_columns, \"string\")\n",
    "rev_df['comments'] = rev_df['comments'].astype(\"string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Handle Percentage Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert percentage columns\n",
    "percentage_cols = ['host_response_rate', 'host_acceptance_rate'] # used later for response stats\n",
    "for col in percentage_cols:\n",
    "    lis_df = lis_df.rename(columns={col: f\"{col}_pct\"})\n",
    "    lis_df[f\"{col}_pct\"] = lis_df[f\"{col}_pct\"].str.rstrip('%').astype('float') / 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Process array-Type Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process amenities\n",
    "lis_df['amenities_count'] = lis_df.amenities.str.strip('[]').str.split(',').str.len()\n",
    "\n",
    "def clean_amenity(text):\n",
    "    \"\"\"Clean individual amenity strings\"\"\"\n",
    "    import re\n",
    "    text = str(text) # Convert to string if not already\n",
    "    text = text.strip().strip('\"\\'').strip('.- ') # Basic cleaning\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii') # Replace unicode escape sequences with their characters\n",
    "    text = re.sub(r'\\s+', ' ', text) # Replace multiple spaces with single space\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s\\-.,:/+&æøåÆØÅ]', '', text) # Remove special characters, keeping only alphanumeric and spaces\n",
    "    text = text.lower().strip() # Convert to lowercase, strip again, and remove any remaining leading/trailing spaces\n",
    "    return text\n",
    "\n",
    "# Clean and convert amenities to comma-separated string\n",
    "lis_df['amenities'] = lis_df['amenities'].str.strip('[]').str.split(',').apply(\n",
    "    lambda x: ','.join(\n",
    "        sorted(  # Sort for consistency\n",
    "            filter(None,  # Remove empty strings\n",
    "                [clean_amenity(item) for item in x]\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "lis_df['amenities'] = lis_df['amenities'].astype('string') # Convert to string dtype\n",
    "\n",
    "# Process host verifications. Count number of verifications per host\n",
    "lis_df['host_verifications_count'] = lis_df['host_verifications'].str.strip('[]').str.split(', ').str.len()\n",
    "\n",
    "lis_df.drop(columns=['host_verifications', 'amenities'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Handle Missing Values (Imputation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to analyze missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_missing_values(df, df_name):\n",
    "    # Calculate missing values\n",
    "    missing = df.isnull().sum()\n",
    "    missing_percent = (df.isnull().sum() / len(df)) * 100\n",
    "    \n",
    "    # Create a summary DataFrame\n",
    "    missing_info = pd.DataFrame({\n",
    "        'Missing Values': missing,\n",
    "        'Missing Percentage': missing_percent.round(2)\n",
    "    })\n",
    "    \n",
    "    # Only show columns with missing values, sorted by percentage\n",
    "    missing_info = missing_info[missing_info['Missing Values'] > 0].sort_values(\n",
    "        'Missing Percentage', ascending=False\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nMissing Values Analysis for {df_name}:\")\n",
    "    print(\"-\" * 50)\n",
    "    if len(missing_info) > 0:\n",
    "        print(missing_info)\n",
    "    else:\n",
    "        print(\"No missing values found!\")\n",
    "    print(f\"\\nTotal rows in dataset: {len(df)}\")\n",
    "\n",
    "# Analyze both datasets\n",
    "analyze_missing_values(lis_df, \"Listings\")\n",
    "analyze_missing_values(rev_df, \"Reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Price Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix price columns\n",
    "#DKK\n",
    "lis_df['price'] = lis_df['price'].str.replace(r'[\\$,]', '', regex=True)\n",
    "lis_df = lis_df.rename(columns={'price': 'price_DKK'})\n",
    "lis_df['price_DKK'] = pd.to_numeric(lis_df['price_DKK'], errors='coerce')\n",
    "\n",
    "# Drop the 4 most expensive listings (they looked like ingenuine outliers)\n",
    "expensive_indices = lis_df['price_DKK'].nlargest(4).index\n",
    "lis_df = lis_df.drop(expensive_indices)\n",
    "\n",
    "#USD\n",
    "cal_df['price'] = cal_df['price'].str.replace(r'[\\$,]', '', regex=True)\n",
    "cal_df = cal_df.rename(columns={'price': 'price_USD'})\n",
    "cal_df['price_USD'] = pd.to_numeric(cal_df['price_USD'], errors='coerce')\n",
    "\n",
    "\n",
    "# Prepare average prices per listing\n",
    "cal_prices = cal_df.groupby('listing_id')['price_USD'].mean().reset_index()\n",
    "# Multiply prices under 500 by 5.9\n",
    "cal_prices.loc[cal_prices['price_USD'] < 500, 'price_USD'] *= 5.9\n",
    "\n",
    "# Merge with listings\n",
    "lis_df = lis_df.merge(\n",
    "    cal_prices,\n",
    "    left_on='id',\n",
    "    right_on='listing_id',\n",
    "    how='left'\n",
    ")\n",
    "lis_df.drop('listing_id', axis=1, inplace=True)\n",
    "\n",
    "# Fill missing DKK prices with USD prices\n",
    "lis_df.loc[lis_df['price_DKK'].isna(), 'price_DKK'] = lis_df.loc[lis_df['price_DKK'].isna(), 'price_USD']\n",
    "# Drop price_USD if no missing values in price_DKK\n",
    "if not lis_df['price_DKK'].isna().any():\n",
    "    lis_df.drop('price_USD', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Listing Data Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Host fields\n",
    "host_cols_to_impute = {\n",
    "    'host_acceptance_rate_pct': 0.0,  #\n",
    "    'host_total_listings_count': 1,  #\n",
    "    'host_verifications_count': 0,  # \n",
    "    'host_response_time': '',  # \n",
    "    'host_since': pd.NaT  # Used to calculate host_experience_years\n",
    "}\n",
    "\n",
    "for col, value in host_cols_to_impute.items():\n",
    "    lis_df[col] = lis_df[col].fillna(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review score fields - impute with median values\n",
    "review_score_cols = [col for col in lis_df.columns if col.startswith('review_scores_')]\n",
    "for col in review_score_cols:\n",
    "    median_score = lis_df[col].median()\n",
    "    lis_df[col] = lis_df[col].fillna(median_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_df['bathrooms_text'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle half baths and NAs in bathrooms_text\n",
    "# First convert to float to avoid dtype warning\n",
    "half_bath_mask = lis_df['bathrooms_text'].isin(['Half-bath', 'Shared half-bath', 'Private half-bath'])\n",
    "lis_df.loc[half_bath_mask, 'bathrooms_text'] = '0.5'\n",
    "\n",
    "# Extract numeric values from bathrooms_text\n",
    "# Convert to string first before using str accessor\n",
    "lis_df['bathrooms_text'] = lis_df['bathrooms_text'].astype(str).str.extract(r'^(\\d+\\.?\\d?)').astype(float)\n",
    "\n",
    "# Handle NAs with explicit float conversion\n",
    "lis_df['bathrooms_text'] = lis_df['bathrooms_text'].astype(float).fillna(0.0)\n",
    "\n",
    "# Fill NaN bathrooms values with bathrooms_text values\n",
    "lis_df.loc[lis_df['bathrooms'].isna(), 'bathrooms'] = lis_df.loc[lis_df['bathrooms'].isna(), 'bathrooms_text']\n",
    "print(len(lis_df['bathrooms_text'].unique()), lis_df['bathrooms_text'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accommodation fields\n",
    "accommodation_cols_to_impute = {\n",
    "    'room_type': pd.NA,    \n",
    "    'property_type': pd.NA,\n",
    "    'amenities_count': 0,\n",
    "    'accommodates': 1,  \n",
    "    'has_availability': False,\n",
    "    'instant_bookable': False \n",
    "}\n",
    "\n",
    "for col, value in accommodation_cols_to_impute.items():\n",
    "    lis_df[col] = lis_df[col].fillna(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Response Time Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify host_response_rate_pct is always 0 when host_response_time is empty\n",
    "empty_response_time = lis_df[lis_df['host_response_time'] == '']\n",
    "print(\"\\nHost response rates when response time is empty:\")\n",
    "print(empty_response_time['host_response_rate_pct'].value_counts())\n",
    "print(f\"\\nAll zeros? {(empty_response_time['host_response_rate_pct'] == 0).all()}\")\n",
    "\n",
    "# Remap empty strings to 'never'\n",
    "lis_df.loc[lis_df['host_response_time'] == '', 'host_response_time'] = 'never'\n",
    "\n",
    "lis_df['host_response_time'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review Data Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the missing values are not missing in the reviews data, we will not impute them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_df.to_parquet('data/processed/02_listings.parquet')\n",
    "rev_df.to_parquet('data/processed/02_reviews.parquet')\n",
    "print('rev:',rev_df.shape)\n",
    "print('lis:',lis_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " —————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reasoning for Currency Issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lis_df[['listing_url', 'price_DKK', 'price_USD']][:20]\n",
    "# Check missing DKK prices where USD price > 250\n",
    "# missing_dkk = lis_df[lis_df['price_DKK'].isna()]\n",
    "# missing_dkk_high_usd = missing_dkk[missing_dkk['price_USD'] < 500]\n",
    "# print(f\"Out of {len(missing_dkk)} listings with missing DKK prices,\")\n",
    "# print(f\"{len(missing_dkk_high_usd)} have USD prices < 500\")\n",
    "\n",
    "# Extract listing IDs from URLs and check their review dates\n",
    "# missing_dkk_listings = lis_df.loc[missing_dkk_high_usd.index, ['listing_url', 'price_DKK', 'price_USD', 'first_review', 'last_review']]\n",
    "# missing_dkk_listings = missing_dkk_listings.sort_values('first_review')\n",
    "\n",
    "# print(\"First listing's last review:\", missing_dkk_listings['first_review'].iloc[0])\n",
    "# print(\"Last listing's last review:\", missing_dkk_listings['last_review'].iloc[0])\n",
    "# # missing_dkk_listings[['listing_url', 'price_DKK', 'price_USD']]\n",
    "\n",
    "# lis_df.loc[missing_dkk_high_usd.index, ['listing_url', 'price_DKK', 'price_USD']]\n",
    "\n",
    "# Compare price columns\n",
    "# price_df = lis_df[lis_df.columns[lis_df.columns.str.startswith('price')]]\n",
    "\n",
    "# # Count missing values\n",
    "# missing_both = price_df[['price_DKK', 'price_USD']].isna().all(axis=1).sum()\n",
    "# missing_dkk_only = price_df['price_DKK'].isna().sum() - missing_both\n",
    "# missing_usd_only = price_df['price_USD'].isna().sum() - missing_both\n",
    "\n",
    "# # Count same/different prices (excluding missing)\n",
    "# valid_prices = price_df.dropna()\n",
    "# same_prices = (valid_prices['price_DKK'] == valid_prices['price_USD']).sum()\n",
    "# diff_prices = len(valid_prices) - same_prices\n",
    "\n",
    "# print(f\"Missing both prices: {missing_both}\")\n",
    "# print(f\"Missing only DKK price: {missing_dkk_only}\")\n",
    "# print(f\"Missing only USD price: {missing_usd_only}\")\n",
    "\n",
    "# print(f\"Number of listings with same prices in DKK and USD: {same_prices}\")\n",
    "# print(f\"Number of listings with different prices in DKK and USD: {diff_prices}\")\n",
    "# same_prices = (valid_prices['price_DKK'] == valid_prices['price_USD']).sum()\n",
    "# diff_prices = len(valid_prices) - same_prices\n",
    "\n",
    "\n",
    "# # Check how many rows with missing DKK prices have USD prices available\n",
    "# missing_dkk = lis_df[lis_df['price_DKK'].isna()]\n",
    "# dkk_missing_usd_available = missing_dkk['price_USD'].notna().sum()\n",
    "\n",
    "# print(f\"Out of {len(missing_dkk)} rows with missing DKK prices,\")\n",
    "# print(f\"{dkk_missing_usd_available} have USD prices available\")\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # Calculate 1st and 99th percentiles\n",
    "# lower_bound = lis_df['price_USD'].quantile(0.05)\n",
    "# upper_bound = lis_df['price_USD'].quantile(0.95)\n",
    "\n",
    "# # Filter out extreme outliers (1%)\n",
    "# clean_prices = lis_df['price_USD'][(lis_df['price_USD'] >= lower_bound) & \n",
    "#                                  (lis_df['price_USD'] <= upper_bound)]\n",
    "\n",
    "# # Plot distribution of cleaned USD prices\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.hist(clean_prices, bins=50)\n",
    "# plt.title('Distribution of USD Prices (5% Extreme Outliers Removed)')\n",
    "# plt.xlabel('Price (USD)')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n",
    "\n",
    "# # Print summary statistics\n",
    "# print(\"\\nSummary statistics for cleaned USD prices:\")\n",
    "# print(clean_prices.describe())\n",
    "# print(f\"\\nRemoved {len(lis_df['price_USD'].dropna()) - len(clean_prices)} outliers\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
